---
title: "Daoyang E Week 1 Text Mining"
output: html_notebook
---

# Step 0: check and install needed packages. Load the libraries and functions. 

```{r message=FALSE, warning=FALSE,include=FALSE}
packages.used=c("rvest", "tibble", "sentimentr", "gplots", "dplyr", "qdap",
                "tm", "syuzhet", "factoextra", "beeswarm", "scales", "RColorBrewer",
                "RANN", "tm", "topicmodels","readtext", "wordcloud", "ggplot2", 
                "purrr", "readr", "tidytext", "knitr", "shiny")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))

# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library("rvest")
library("tibble")
library("wordcloud")
library("qdap")
library("sentimentr")
library("gplots")
library("dplyr")
library("tm")
library("syuzhet")
library("factoextra")
library("beeswarm")
library("scales")
library("RColorBrewer")
library("RANN")
library("tm")
library("topicmodels")
library("readtext")
library("ggplot2")
library("purrr")
library("readr")
library("tidytext")
library("knitr")
library("shiny")
```


```{r}
#show R version
print(R.version)
```


```{r, message=FALSE}
#import data and save it to dataframe named philosophy_data
library(readr)
philosophy_data <- read_csv("../data/philosophy_data.csv")
```


```{r}
#overview of the data structure
str(philosophy_data)
```

# Step 1, take a general look at the graphs of philosophy data

```{r}
#count the number of tokens for each sentence
philosophy_data$ntokens <- unlist(map(map(map(philosophy_data$tokenized_txt, 
                                              strsplit, "', '"),
                                          unlist),
                                      length))
```

```{r}
#plot the count of sentences based on book titles
ggplot(data = philosophy_data,
       aes(x = title))+
  geom_bar(width = 0.4,
           position = position_dodge(width = 0.5))+
  labs(title = "count of sentences based on book titles")+
  theme(axis.text.x = element_text(angle = 90))
```



```{r}
#plot the count of sentences based on author name
ggplot(data = philosophy_data,
       aes(x = author))+
  geom_bar(width = 0.4,
           position = position_dodge(width = 0.5))+
  labs(title = "count of sentences based on author name")+
  theme(axis.text.x = element_text(angle = 90))
```


```{r}
#plot the count of sentences based on philosophy schools
ggplot(data = philosophy_data,
       aes(x = school))+
  geom_bar(width = 0.4,
           position = position_dodge(width=0.5))+
  labs(title = "count of sentences based on philosophy schools")+
  theme(axis.text.x = element_text(angle = 90))
```


```{r}
#plot the count of sentence length based on philosophy schools
ggplot(data = philosophy_data, aes(x = school, y = sentence_length))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 90))+
  labs(title = "count of sentence length based on philosophy schools")
```


```{r}
#plot the count of tokens based on philosophy schools
ggplot(data = philosophy_data, aes(x = school, 
                                   y = ntokens))+
  geom_boxplot()+
  theme(axis.text.x = element_text(angle = 90))+
  labs(title = "count of tokens based on philosophy schools")
```


From the graphs, we can see that among all the authors, Aristotle and Plato apparently possess more works than other authors. However, the most prevalent school appears to be analytic, number of works from analytic school surpasses other schools.


```{r}
#group philosophy data by school and author and compute the mean of sentence length and number of tokens
mean_analysis <- philosophy_data %>% 
  group_by(author, school) %>% 
  summarise(sentence_length_mean = mean(sentence_length), 
            ntokens_mean = mean(ntokens)) %>% arrange(desc(sentence_length_mean))

#print the top six lines arranged by mean of sentence length descending order
head(mean_analysis)
```


Concerning sentence length and number of tokens, empiricism and rationalism appear to possess longer sentence length and more tokens than other schools.


# 3. Doing sentiment analysis


```{r}
#randomly sample 10% of the data since my computer is not able to process such big data
set.seed(10086)
sample <- sample_frac(philosophy_data, 0.1)
```


```{r}
#reformatting the sentences and add 8 columns of sentiment to the data frame
sentence.list = NULL
for(i in 1:nrow(sample)){
  sentences = sent_detect(sample$sentence_spacy[i],
                        endmarks = c("?", ".", "!", "|",";"))
  if(length(sentences) > 0){  
    emotions = get_nrc_sentiment(sentences)
    word.count = word_count(sentences)
    emotions = (1 / (word.count + 0.01) * as.matrix(emotions))
    sentence.list = rbind(sentence.list, 
                        cbind(sample[i, -ncol(sample)],
                              sentences = as.character(sentences), 
                              word.count,
                              emotions,
                              sent.id = 1:length(sentences)
                              )
    )
  }
}

#remove the sentence where word count is 0
sentence.list <-
  sentence.list %>%
  filter(!is.na(word.count))
```


```{r}
#same the name of sentiments to a tibble named sentiments
sentiments <- as_tibble(c("anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust"))

#find unique school names and save it to a vector school
schools <- unique(philosophy_data$school)
```


```{r warning=FALSE}
#print the sentence having the greatest eight emotions by school 
for (i in schools) {
  print(i)
  sentence.df = as_tibble(sentence.list) %>%
  filter(school == i, 
         word.count >= 4) %>%
  select(sentences, 
         anger:trust)
  sentence.df = as.data.frame(sentence.df)
  max.sentiment <- as.character(sentence.df$sentences[apply(sentence.df[,-1], 2, which.max)])
  print(cbind(sentiments,
       max.sentiment))
}
```

From the result, I can see that apparently different schools centers on different kind of sentiments. For example. plato, aistotle and german_idealism appears to have one sentence repeating several times for negative sentiments, which I assume that it contains rather strong means. These sentences possessing strong sentiments generally do not show same words, thus I believe that in this case these schools used nonoverlapping set of words to illustrate their points


# 4. Making wordclouds


```{r}
plato <- sample %>% filter(school == "plato")
write.table(plato$sentence_spacy, file = "../data/schools/plato.txt", sep = "", row.names = FALSE)
aristotle <- sample %>% filter(school == "aristotle")
write.table(aristotle$sentence_spacy, file = "../data/schools/aristotle.txt", sep = "", row.names = FALSE)
empiricism <- sample %>% filter(school == "empiricism")
write.table(empiricism$sentence_spacy, file = "../data/schools/empiricism.txt", sep = "", row.names = FALSE)
rationalism <- sample %>% filter(school == "rationalism")
write.table(rationalism$sentence_spacy, file = "../data/schools/rationalism.txt", sep = "", row.names = FALSE)
analytic <- sample %>% filter(school == "analytic")
write.table(analytic$sentence_spacy, file = "../data/schools/analytic.txt", sep = "", row.names = FALSE)
continental <- sample %>% filter(school == "continental")
write.table(continental$sentence_spacy, file = "../data/schools/continental.txt", sep = "", row.names = FALSE)
phenomenology <- sample %>% filter(school == "phenomenology")
write.table(phenomenology$sentence_spacy, file = "../data/schools/phenomenology.txt", sep = "", row.names = FALSE)
german_idealism <- sample %>% filter(school == "german_idealism")
write.table(german_idealism$sentence_spacy, file = "../data/schools/german_idealism.txt", sep = "", row.names = FALSE)
communism <- sample %>% filter(school == "communism")
write.table(communism$sentence_spacy, file = "../data/schools/communism.txt", sep = "", row.names = FALSE)
capitalism <- sample %>% filter(school == "capitalism")
write.table(capitalism$sentence_spacy, file = "../data/schools/capitalism.txt", sep = "", row.names = FALSE)
stoicism <- sample %>% filter(school == "stoicism")
write.table(stoicism$sentence_spacy, file = "../data/schools/stoicism.txt", sep = "", row.names = FALSE)
nietzsche <- sample %>% filter(school == "nietzsche")
write.table(nietzsche$sentence_spacy, file = "../data/schools/nietzsche.txt", sep = "", row.names = FALSE)
feminism <- sample %>% filter(school == "feminism")
write.table(feminism$sentence_spacy, file = "../data/schools/feminism.txt", sep = "", row.names = FALSE)
```



```{r}
folder.path="../data/schools/"
schools=list.files(path = folder.path, pattern = "*.txt")
prex.out=substr(schools, 6, nchar(schools)-4)
ff.all<-Corpus(DirSource(folder.path))
```

```{r}
ff.all<-tm_map(ff.all, stripWhitespace)
ff.all<-tm_map(ff.all, content_transformer(tolower))
ff.all<-tm_map(ff.all, removeWords, stopwords("english"))
ff.all<-tm_map(ff.all, removeWords, character(0))
ff.all<-tm_map(ff.all, removePunctuation)
ff.all
tdm.all<-TermDocumentMatrix(ff.all)
tdm.all

# Turn the Term Document Matrix into a tidy tibble
tdm.tidy=tidy(tdm.all)
kable(tdm.tidy[1:10,])
# Get the Overall Counts over the Whole Corpus
tdm.overall=summarise(group_by(tdm.tidy, term), sum(count))
kable(tdm.overall[101:110,])
```



```{r, fig.height=6, fig.width=6}
#an overall wordcloud
wordcloud(tdm.overall$term, tdm.overall$`sum(count)`,
          scale=c(5,0.5),
          max.words=100,
          min.freq=1,
          random.order=FALSE,
          rot.per=0.3,
          random.color=FALSE,
          colors=brewer.pal(9,"Blues"))
```



```{r, warning=FALSE}
#compute TF-IDF weighted document-term matrices for individual schools. 
dtm <- DocumentTermMatrix(ff.all,
                          control = list(weighting = function(x) weightTfIdf(x,normalize=FALSE),
                                         stopwords = TRUE))
ff.dtm=tidy(dtm)
ff.dtm <- ff.dtm %>% na.omit()
```

```{r warning=FALSE}
library(shiny)
shinyApp(
    ui = fluidPage(
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('school1', 'School1',
                              schools,
                              selected=schools[5])),
        column(4, selectInput('school2', 'School2', schools,
                              selected=schools[9])),
        column(4, sliderInput('nwords', 'Number of words', 3,
                               min = 20, max = 200, value=100, step = 20))
      ),
      fluidRow(
        plotOutput('wordclouds', height = "400px")
      )
    ),
    server = function(input, output, session) {
      # Combine the selected variables into a new data frame
      selectedData <- reactive({
        list(dtm.term1=ff.dtm$term[ff.dtm$document==as.character(input$school1)],
             dtm.count1=ff.dtm$count[ff.dtm$document==as.character(input$school1)],
             dtm.term2=ff.dtm$term[ff.dtm$document==as.character(input$school2)],
             dtm.count2=ff.dtm$count[ff.dtm$document==as.character(input$school2)])
      })
      output$wordclouds <- renderPlot(height = 400, {
        par(mfrow=c(1,2), mar = c(0, 0, 3, 0))
        wordcloud(selectedData()$dtm.term1, 
                  selectedData()$dtm.count1,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0.3,
              random.color=FALSE,
              colors=brewer.pal(9,"Blues"), 
            main=input$speech1)
        wordcloud(selectedData()$dtm.term2, 
                  selectedData()$dtm.count2,
              scale=c(4,0.5),
              max.words=input$nwords,
              min.freq=1,
              random.order=FALSE,
              rot.per=0.3,
              random.color=FALSE,
              colors=brewer.pal(9,"Blues"), 
            main=input$speech2)
      })
    },
    options = list(height = 600)
)
```

